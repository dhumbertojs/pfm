{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/clean_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(n=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palabras tokens a excluir\n",
    "palabras_excluir = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = df['descripcion_solicitud'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia de los tópicos: 0.3928203431931022\n",
      "Tema: 0\n",
      "Palabras clave: 0.010*\"pública\" + 0.008*\"ley\" + 0.007*\"transparencia\" + 0.006*\"personal\" + 0.006*\"general\" + 0.006*\"nombre\" + 0.006*\"gobierno\" + 0.005*\"copia\" + 0.005*\"artículo\" + 0.005*\"así\"\n",
      "\n",
      "Tema: 1\n",
      "Palabras clave: 0.010*\"empresas\" + 0.009*\"servicio\" + 0.009*\"copia\" + 0.007*\"limpieza\" + 0.007*\"carpeta\" + 0.007*\"línea\" + 0.006*\"requiere\" + 0.006*\"policía\" + 0.006*\"obligado\" + 0.005*\"sujeto\"\n",
      "\n",
      "Tema: 2\n",
      "Palabras clave: 0.010*\"fecha\" + 0.008*\"copia\" + 0.008*\"delegacion\" + 0.007*\"año\" + 0.006*\"construcción\" + 0.006*\"obra\" + 0.005*\"uso\" + 0.005*\"gobierno\" + 0.005*\"numero\" + 0.005*\"informacion\"\n",
      "\n",
      "Tema: 3\n",
      "Palabras clave: 0.010*\"año\" + 0.009*\"presupuesto\" + 0.007*\"número\" + 0.006*\"gobierno\" + 0.006*\"personas\" + 0.006*\"fecha\" + 0.005*\"años\" + 0.005*\"monto\" + 0.005*\"total\" + 0.004*\"delegación\"\n",
      "\n",
      "Tema: 4\n",
      "Palabras clave: 0.009*\"delegación\" + 0.007*\"colonia\" + 0.005*\"número\" + 0.005*\"calle\" + 0.005*\"caso\" + 0.004*\"centro\" + 0.004*\"programa\" + 0.004*\"alcaldía\" + 0.004*\"ser\" + 0.003*\"proyecto\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "custom_stop_words = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\",\n",
    "                    \",\", \".\", \"?\", \"/\", \":\", \"*\", \";\", \"_\", \"-\",\n",
    "                    \"``\", \"''\", \"si\"]  # Agrega aquí las palabras adicionales a excluir\n",
    "stop_words.update(custom_stop_words)\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
    "\n",
    "# Crear el diccionario\n",
    "dictionary = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Crear el corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_data]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "num_topics = 5  # Número de tópicos que deseas extraer\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Evaluación de la coherencia de los tópicos\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=filtered_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherencia de los tópicos:\", coherence_score)\n",
    "\n",
    "# Imprimir los tópicos y palabras clave\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Tema: {}\\nPalabras clave: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia de los tópicos: 0.42961815609548815\n",
      "Tema: 0\n",
      "Palabras clave: 0.007*\"caso\" + 0.007*\"personal\" + 0.006*\"ser\" + 0.005*\"así\" + 0.004*\"datos\" + 0.004*\"servicio\" + 0.004*\"día\" + 0.004*\"personas\" + 0.004*\"cuenta\" + 0.004*\"público\"\n",
      "\n",
      "Tema: 1\n",
      "Palabras clave: 0.016*\"año\" + 0.014*\"número\" + 0.012*\"total\" + 0.011*\"personas\" + 0.011*\"enero\" + 0.010*\"diga\" + 0.009*\"2019\" + 0.009*\"mes\" + 0.009*\"cuántas\" + 0.009*\"años\"\n",
      "\n",
      "Tema: 2\n",
      "Palabras clave: 0.024*\"gobierno\" + 0.014*\"presupuesto\" + 0.014*\"año\" + 0.009*\"fecha\" + 0.008*\"programa\" + 0.007*\"programas\" + 0.007*\"delegación\" + 0.006*\"personas\" + 0.006*\"delegacion\" + 0.006*\"secretaria\"\n",
      "\n",
      "Tema: 3\n",
      "Palabras clave: 0.017*\"contratos\" + 0.013*\"contrato\" + 0.008*\"monto\" + 0.008*\"servicios\" + 0.008*\"ejercicio\" + 0.007*\"empresa\" + 0.007*\"partida\" + 0.007*\"número\" + 0.007*\"s.a.\" + 0.007*\"nombre\"\n",
      "\n",
      "Tema: 4\n",
      "Palabras clave: 0.015*\"pública\" + 0.011*\"copia\" + 0.010*\"general\" + 0.008*\"ley\" + 0.007*\"transparencia\" + 0.006*\"artículo\" + 0.006*\"documento\" + 0.006*\"fecha\" + 0.006*\"acceso\" + 0.005*\"“\"\n",
      "\n",
      "Tema: 5\n",
      "Palabras clave: 0.014*\"%\" + 0.011*\"nombre\" + 0.011*\"delito\" + 0.010*\"sujeto\" + 0.009*\"investigación\" + 0.009*\"obligado\" + 0.008*\"número\" + 0.008*\"limpieza\" + 0.008*\"carpeta\" + 0.008*\"empresas\"\n",
      "\n",
      "Tema: 6\n",
      "Palabras clave: 0.019*\"colonia\" + 0.014*\"delegación\" + 0.014*\"calle\" + 0.012*\"ubicado\" + 0.010*\"san\" + 0.009*\"copia\" + 0.009*\"predio\" + 0.008*\"número\" + 0.007*\"uso\" + 0.007*\"suelo\"\n",
      "\n",
      "Tema: 7\n",
      "Palabras clave: 0.011*\"delegación\" + 0.010*\"construcción\" + 0.010*\"obra\" + 0.009*\"proyecto\" + 0.007*\"obras\" + 0.006*\"pública\" + 0.006*\"y/o\" + 0.006*\"agua\" + 0.005*\"número\" + 0.005*\"así\"\n",
      "\n",
      "Tema: 8\n",
      "Palabras clave: 0.010*\"nombre\" + 0.010*\"cargo\" + 0.009*\"fecha\" + 0.009*\"c.\" + 0.008*\"puesto\" + 0.007*\"personal\" + 0.007*\"sueldo\" + 0.007*\"informe\" + 0.006*\"dependencia\" + 0.005*\"así\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "custom_stop_words = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\",\n",
    "                    \",\", \".\", \"?\", \"/\", \":\", \"*\", \";\", \"_\", \"-\",\n",
    "                    \"``\", \"''\", \"si\"]  # Agrega aquí las palabras adicionales a excluir\n",
    "stop_words.update(custom_stop_words)\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
    "\n",
    "# Crear el diccionario\n",
    "dictionary = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Crear el corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_data]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "num_topics = 9  # Número de tópicos que deseas extraer\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Evaluación de la coherencia de los tópicos\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=filtered_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherencia de los tópicos:\", coherence_score)\n",
    "\n",
    "# Imprimir los tópicos y palabras clave\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Tema: {}\\nPalabras clave: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia de los tópicos: 0.48114156068621694\n",
      "Tema: 0\n",
      "Palabras clave: 0.018*\"datos\" + 0.011*\"personales\" + 0.010*\"sistema\" + 0.009*\"investigación\" + 0.009*\"requiere\" + 0.009*\"legal\" + 0.009*\"fundamento\" + 0.009*\"carpeta\" + 0.008*\"delito\" + 0.008*\"¿cuál\"\n",
      "\n",
      "Tema: 1\n",
      "Palabras clave: 0.009*\"luis\" + 0.008*\"hospital\" + 0.008*\"parque\" + 0.007*\"estación\" + 0.006*\"vehicular\" + 0.006*\"puestos\" + 0.006*\"adolescentes\" + 0.006*\"gonzalez\" + 0.006*\"impuesto\" + 0.006*\"ambulantes\"\n",
      "\n",
      "Tema: 2\n",
      "Palabras clave: 0.010*\"personas\" + 0.008*\"programa\" + 0.006*\"presupuesto\" + 0.006*\"programas\" + 0.006*\"centro\" + 0.005*\"%\" + 0.005*\"gobierno\" + 0.005*\"¿qué\" + 0.005*\"y/o\" + 0.005*\"caso\"\n",
      "\n",
      "Tema: 3\n",
      "Palabras clave: 0.010*\"respuesta\" + 0.008*\"medio\" + 0.008*\"gracias\" + 0.007*\"solicitar\" + 0.007*\"pública\" + 0.006*\"manera\" + 0.006*\"constitución\" + 0.006*\"acceso\" + 0.005*\"mexicanos\" + 0.005*\"vía\"\n",
      "\n",
      "Tema: 4\n",
      "Palabras clave: 0.026*\"año\" + 0.016*\"número\" + 0.014*\"total\" + 0.013*\"mes\" + 0.013*\"enero\" + 0.012*\"2010\" + 0.010*\"2012\" + 0.010*\"2008\" + 0.010*\"2015\" + 0.010*\"2009\"\n",
      "\n",
      "Tema: 5\n",
      "Palabras clave: 0.013*\"copia\" + 0.011*\"fecha\" + 0.007*\"ubicado\" + 0.007*\"número\" + 0.007*\"nombre\" + 0.007*\"predio\" + 0.007*\"oficio\" + 0.006*\"registro\" + 0.006*\"san\" + 0.006*\"expediente\"\n",
      "\n",
      "Tema: 6\n",
      "Palabras clave: 0.021*\"gobierno\" + 0.012*\"fecha\" + 0.010*\"secretaria\" + 0.007*\"delegacion\" + 0.007*\"informe\" + 0.007*\"general\" + 0.007*\"transporte\" + 0.005*\"realizado\" + 0.005*\"jefe\" + 0.005*\"medio\"\n",
      "\n",
      "Tema: 7\n",
      "Palabras clave: 0.017*\"copia\" + 0.016*\"contratos\" + 0.013*\"contrato\" + 0.012*\"servicio\" + 0.011*\"servicios\" + 0.010*\"empresas\" + 0.010*\"versión\" + 0.010*\"pública\" + 0.009*\"fecha\" + 0.008*\"así\"\n",
      "\n",
      "Tema: 8\n",
      "Palabras clave: 0.018*\"pública\" + 0.017*\"transparencia\" + 0.015*\"ley\" + 0.015*\"personal\" + 0.011*\"artículo\" + 0.010*\"requiero\" + 0.009*\"general\" + 0.008*\"así\" + 0.008*\"base\" + 0.008*\"acceso\"\n",
      "\n",
      "Tema: 9\n",
      "Palabras clave: 0.024*\"delegación\" + 0.019*\"colonia\" + 0.013*\"construcción\" + 0.013*\"alcaldía\" + 0.013*\"calle\" + 0.010*\"proyecto\" + 0.008*\"agua\" + 0.007*\"y/o\" + 0.007*\"obra\" + 0.007*\"uso\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "custom_stop_words = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\",\n",
    "                    \",\", \".\", \"?\", \"/\", \":\", \"*\", \";\", \"_\", \"-\",\n",
    "                    \"``\", \"''\", \"si\"]  # Agrega aquí las palabras adicionales a excluir\n",
    "stop_words.update(custom_stop_words)\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
    "\n",
    "# Crear el diccionario\n",
    "dictionary = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Crear el corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_data]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "num_topics = 10  # Número de tópicos que deseas extraer\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Evaluación de la coherencia de los tópicos\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=filtered_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherencia de los tópicos:\", coherence_score)\n",
    "\n",
    "# Imprimir los tópicos y palabras clave\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Tema: {}\\nPalabras clave: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia de los tópicos: 0.4770637355526021\n",
      "Tema: 0\n",
      "Palabras clave: 0.028*\"año\" + 0.020*\"mes\" + 0.019*\"total\" + 0.017*\"diga\" + 0.014*\"siguiente\" + 0.014*\"costo\" + 0.013*\"2008\" + 0.012*\"cantidad\" + 0.011*\"enero\" + 0.010*\"2010\"\n",
      "\n",
      "Tema: 1\n",
      "Palabras clave: 0.012*\"personal\" + 0.007*\"nombre\" + 0.007*\"gobierno\" + 0.007*\"caso\" + 0.007*\"cargo\" + 0.007*\"así\" + 0.006*\"c.\" + 0.006*\"fecha\" + 0.006*\"informe\" + 0.006*\"unidad\"\n",
      "\n",
      "Tema: 2\n",
      "Palabras clave: 0.014*\"legal\" + 0.012*\"fundamento\" + 0.011*\"día\" + 0.010*\"carpeta\" + 0.010*\"requiere\" + 0.010*\"¿cuál\" + 0.009*\"investigación\" + 0.008*\"delito\" + 0.008*\"servidores\" + 0.007*\"nombre\"\n",
      "\n",
      "Tema: 3\n",
      "Palabras clave: 0.016*\"ley\" + 0.015*\"pública\" + 0.012*\"transparencia\" + 0.011*\"acceso\" + 0.010*\"artículo\" + 0.007*\"datos\" + 0.007*\"1\" + 0.007*\"fecha\" + 0.007*\"materia\" + 0.006*\"número\"\n",
      "\n",
      "Tema: 4\n",
      "Palabras clave: 0.024*\"personas\" + 0.016*\"%\" + 0.009*\"archivo\" + 0.008*\"violencia\" + 0.008*\"pública\" + 0.007*\"documento\" + 0.007*\"delitos\" + 0.007*\"procuraduría\" + 0.007*\"requiero\" + 0.007*\"edad\"\n",
      "\n",
      "Tema: 5\n",
      "Palabras clave: 0.021*\"delegacion\" + 0.016*\"fecha\" + 0.014*\"alta\" + 0.012*\"informacion\" + 0.009*\"gobierno\" + 0.009*\"presente\" + 0.009*\"año\" + 0.008*\"milpa\" + 0.008*\"referido\" + 0.007*\"realizado\"\n",
      "\n",
      "Tema: 6\n",
      "Palabras clave: 0.017*\"presupuesto\" + 0.014*\"programa\" + 0.012*\"año\" + 0.012*\"monto\" + 0.010*\"contratos\" + 0.008*\"programas\" + 0.007*\"número\" + 0.007*\"contrato\" + 0.007*\"recursos\" + 0.007*\"total\"\n",
      "\n",
      "Tema: 7\n",
      "Palabras clave: 0.018*\"copia\" + 0.016*\"construcción\" + 0.016*\"ubicado\" + 0.015*\"colonia\" + 0.014*\"uso\" + 0.014*\"predio\" + 0.014*\"calle\" + 0.013*\"obra\" + 0.011*\"delegación\" + 0.011*\"permiso\"\n",
      "\n",
      "Tema: 8\n",
      "Palabras clave: 0.024*\"delegación\" + 0.013*\"número\" + 0.013*\"colonia\" + 0.012*\"alcaldía\" + 0.008*\"proyecto\" + 0.007*\"agua\" + 0.006*\"y/o\" + 0.005*\"centro\" + 0.005*\"1\" + 0.005*\"calle\"\n",
      "\n",
      "Tema: 9\n",
      "Palabras clave: 0.023*\"iztacalco\" + 0.014*\"delegacion\" + 0.009*\"libro\" + 0.008*\"diario\" + 0.008*\"perros\" + 0.007*\"control\" + 0.005*\"efectivo\" + 0.005*\"mayor\" + 0.005*\"feria\" + 0.005*\"financiera\"\n",
      "\n",
      "Tema: 10\n",
      "Palabras clave: 0.019*\"copia\" + 0.017*\"servicio\" + 0.015*\"pública\" + 0.013*\"“\" + 0.012*\"empresas\" + 0.012*\"versión\" + 0.011*\"”\" + 0.010*\"servicios\" + 0.009*\"obligado\" + 0.009*\"así\"\n",
      "\n",
      "Tema: 11\n",
      "Palabras clave: 0.009*\"publica\" + 0.008*\"alejandro\" + 0.008*\"comercio\" + 0.007*\"via\" + 0.006*\"comercial\" + 0.006*\"obligados\" + 0.006*\"ambulantes\" + 0.006*\"sujetos\" + 0.005*\"puesto\" + 0.005*\"requiero\"\n",
      "\n",
      "Tema: 12\n",
      "Palabras clave: 0.033*\"gobierno\" + 0.020*\"desarrollo\" + 0.017*\"secretaria\" + 0.011*\"ambiental\" + 0.010*\"medio\" + 0.010*\"programa\" + 0.008*\"ambiente\" + 0.008*\"urbano\" + 0.008*\"acciones\" + 0.008*\"respecto\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "custom_stop_words = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\",\n",
    "                    \",\", \".\", \"?\", \"/\", \":\", \"*\", \";\", \"_\", \"-\",\n",
    "                    \"``\", \"''\", \"si\"]  # Agrega aquí las palabras adicionales a excluir\n",
    "stop_words.update(custom_stop_words)\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
    "\n",
    "# Crear el diccionario\n",
    "dictionary = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Crear el corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_data]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "num_topics = 13  # Número de tópicos que deseas extraer\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Evaluación de la coherencia de los tópicos\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=filtered_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherencia de los tópicos:\", coherence_score)\n",
    "\n",
    "# Imprimir los tópicos y palabras clave\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Tema: {}\\nPalabras clave: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherencia de los tópicos: 0.4065887719435287\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LdaModel' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m coherence_score \u001b[39m=\u001b[39m coherence_model\u001b[39m.\u001b[39mget_coherence()\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCoherencia de los tópicos:\u001b[39m\u001b[39m\"\u001b[39m, coherence_score)\n\u001b[1;32m---> 29\u001b[0m lda_model\u001b[39m.\u001b[39;49mfit(corpus)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Imprimir los tópicos y palabras clave\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m idx, topic \u001b[39min\u001b[39;00m lda_model\u001b[39m.\u001b[39mprint_topics(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LdaModel' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "# Tokenización y eliminación de stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "custom_stop_words = [\"solicito\", \"información\", \"ciudad\", \"méxico\",\n",
    "                    \"distrito\", \"federal\", \"saber\", \"cual\", \"donde\", \n",
    "                    \"esta\", \"como\", \"quien\", \"cuales\", \"cada\",\n",
    "                    \"solicitud\", \"para\", \"cuanto\", \"cuánto\",\n",
    "                    \",\", \".\", \"?\", \"/\", \":\", \"*\", \";\", \"_\", \"-\",\n",
    "                    \"``\", \"''\", \"si\"]  # Agrega aquí las palabras adicionales a excluir\n",
    "stop_words.update(custom_stop_words)\n",
    "tokenized_data = [word_tokenize(text.lower()) for text in text_data]\n",
    "filtered_data = [[word for word in tokens if word not in stop_words] for tokens in tokenized_data]\n",
    "\n",
    "# Crear el diccionario\n",
    "dictionary = corpora.Dictionary(filtered_data)\n",
    "\n",
    "# Crear el corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in filtered_data]\n",
    "\n",
    "# Entrenar el modelo LDA\n",
    "num_topics = 15  # Número de tópicos que deseas extraer\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n",
    "\n",
    "# Evaluación de la coherencia de los tópicos\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=filtered_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(\"Coherencia de los tópicos:\", coherence_score)\n",
    "\n",
    "# Imprimir los tópicos y palabras clave\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Tema: {}\\nPalabras clave: {}\\n'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dhumb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NMF.__init__() got an unexpected keyword argument 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Configurar el modelo NMF\u001b[39;00m\n\u001b[0;32m     19\u001b[0m num_topics \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m  \u001b[39m# Número de tópicos deseados\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m nmf_model \u001b[39m=\u001b[39m NMF(n_components\u001b[39m=\u001b[39;49mnum_topics, alpha\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, beta\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m \u001b[39m# Entrenar el modelo NMF\u001b[39;00m\n\u001b[0;32m     23\u001b[0m nmf_model\u001b[39m.\u001b[39mfit(tfidf_matrix)\n",
      "\u001b[1;31mTypeError\u001b[0m: NMF.__init__() got an unexpected keyword argument 'alpha'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar las stop words en español de nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Obtener los textos de la columna 'descripcion_solicitud' del DataFrame\n",
    "text_data = df['descripcion_solicitud'].tolist()\n",
    "\n",
    "# Crear una matriz TF-IDF de términos\n",
    "stop_words = stopwords.words('spanish')  # Utilizar la lista de stop words en español\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "\n",
    "# Configurar el modelo NMF\n",
    "num_topics = 5  # Número de tópicos deseados\n",
    "nmf_model = NMF(n_components=num_topics, alpha=0.1, beta=0.2)\n",
    "\n",
    "# Entrenar el modelo NMF\n",
    "nmf_model.fit(tfidf_matrix)\n",
    "\n",
    "# Obtener las palabras clave de los tópicos\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "topic_keywords = []\n",
    "for topic_weights in nmf_model.components_:\n",
    "    top_keyword_indices = topic_weights.argsort()[:-6:-1]\n",
    "    topic_keywords.append([feature_names[i] for i in top_keyword_indices])\n",
    "\n",
    "# Imprimir los tópicos y sus palabras clave asociadas\n",
    "for topic_idx, topic in enumerate(topic_keywords):\n",
    "    print(f\"Tópico {topic_idx + 1}:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
